{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nipy\n",
    "from nipy import load_image\n",
    "import numpy as np\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#string : name of imange : size -> 256 * 256 * 256\n",
    "#find the bound of organ in brain\n",
    "def find_minmax(string):\n",
    "    img = nib.load(string)\n",
    "    img_data = img.get_data()\n",
    "    X=[256,0]\n",
    "    Y=[256,0]\n",
    "    Z=[256,0]\n",
    "    for x in range(256):\n",
    "        for y in range(256):\n",
    "            for z in range(256):\n",
    "                if img_data[x][y][z]==1:\n",
    "                    X[0] = min(X[0],x)\n",
    "                    X[1] = max(X[1],x)\n",
    "                    Y[0] = min(Y[0],y)\n",
    "                    Y[1] = max(Y[1],y)\n",
    "                    Z[0] = min(Z[0],z)\n",
    "                    Z[1] = max(Z[1],z)\n",
    "    return X,Y,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in_name : name of input image\n",
    "#out_name : name of output image\n",
    "\n",
    "#in_txt : translate input image into in_txt file\n",
    "#out_txt : translate output image into out_txt file\n",
    "\n",
    "def change_into_txtfile(in_name,out_name,size_of_box,in_txt,out_txt):\n",
    "    input_img = nib.load(in_name)\n",
    "    output_img = nib.load(out_name)\n",
    "    X,Y,Z = size_of_box[0],size_of_box[1],size_of_box[2]\n",
    "    \n",
    "    while (X[1]-X[0])%4!=0:\n",
    "        X[1] += 1\n",
    "    while (Y[1]-Y[0])%4!=0:\n",
    "        Y[1] += 1\n",
    "    while (Z[1]-Z[0])%4!=0:\n",
    "        Z[1] += 1\n",
    "    \n",
    "    input_img = input_img.get_data()[X[0]:X[1],Y[0]:Y[1],Z[0]:Z[1]]\n",
    "    output_img = output_img.get_data()[X[0]:X[1],Y[0]:Y[1],Z[0]:Z[1]]\n",
    "    \n",
    "    input_img = np.array(input_img).flatten().tolist()\n",
    "    output_img = np.array(output_img).flatten().tolist()\n",
    "    \n",
    "    with open(in_txt,\"w\") as f:\n",
    "        f.write(str(X[0])+\"\\n\"+str(X[1])+\"\\n\"+str(Y[0])+\"\\n\")\n",
    "        f.write(str(Y[1])+\"\\n\"+str(Z[0])+\"\\n\"+str(Z[1])+\"\\n\")\n",
    "        for x in input_img:\n",
    "            f.write(str(x))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    with open(out_txt,\"w\") as f:\n",
    "        f.write(str(X[0])+\"\\n\"+str(X[1])+\"\\n\"+str(Y[0])+\"\\n\")\n",
    "        f.write(str(Y[1])+\"\\n\"+str(Z[0])+\"\\n\"+str(Z[1])+\"\\n\")\n",
    "        for x in output_img:\n",
    "            f.write(str(x))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 164] [140, 158] [142, 157]\n"
     ]
    }
   ],
   "source": [
    "box = find_minmax(\"Left-Amygdala.nii\")\n",
    "print(box[0],box[1],box[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "box = [[143,164],[140,158],[142,157]]\n",
    "change_into_txtfile(\"T1.nii\",\"Left-Amygdala.nii\",box,\"in.txt\",\"out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.random_uniform(shape, -0.1, 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv3d(x,W):\n",
    "    return tf.nn.conv3d(x,W,strides=[1,1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool_2x2x2(x):\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "def train_nn_for_organism_detection(list_of_file):\n",
    "    import copy\n",
    "    \n",
    "    x_collect = []\n",
    "    y_collect = []\n",
    "    \n",
    "    with open(list_of_file,\"r\") as f:\n",
    "        l = f.read().split()\n",
    "        box_size = copy.deepcopy(l[:6])\n",
    "        X=list(map(int,[box_size[0],box_size[1]]))\n",
    "        Y=list(map(int,[box_size[2],box_size[3]]))\n",
    "        Z=list(map(int,[box_size[4],box_size[5]]))\n",
    "        \n",
    "        for x in range(6,len(l),2):\n",
    "            in_txt = l[x]\n",
    "            out_txt = l[x+1]\n",
    "            \n",
    "            with open(in_txt,\"r\") as f:\n",
    "                l=f.read().split()\n",
    "                x_collect += list(map(float,l[6:]))\n",
    "            with open(out_txt,\"r\") as f:\n",
    "                l=f.read().split()\n",
    "                y_collect += list(map(float,l[6:]))\n",
    "            \n",
    "    shape_of_box = [-1,X[1]-X[0],Y[1]-Y[0],Z[1]-Z[0],1]\n",
    "    size_of_box = (X[1]-X[0])*(Y[1]-Y[0])*(Z[1]-Z[0])\n",
    "    \n",
    "    x = tf.placeholder(\"float\",shape=[None,size_of_box])\n",
    "    y = tf.placeholder(\"float\",shape=[None,size_of_box])\n",
    "    \n",
    "    ####for train#######\n",
    "    \n",
    "    x_image = tf.reshape(x,shape_of_box)\n",
    "    y_image = tf.reshape(y,shape_of_box)\n",
    "    \n",
    "    W_conv1 = weight_variable([5,5,5,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    \n",
    "    h_conv1 = tf.nn.sigmoid(conv3d(x_image,W_conv1)+b_conv1)\n",
    "    h_pool1 = max_pool_2x2x2(h_conv1)\n",
    "    \n",
    "    W_conv2 = weight_variable([5,5,5,32,64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    \n",
    "    h_conv2 = tf.nn.sigmoid(conv3d(h_pool1,W_conv2)+b_conv2)\n",
    "    h_pool2 = max_pool_2x2x2(h_conv2)\n",
    "    \n",
    "    W_fc1 = weight_variable([(size_of_box//8//8)*64,1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1,(size_of_box//8//8)*64])\n",
    "    h_fc1 = tf.nn.sigmoid(tf.matmul(h_pool2_flat, W_fc1)+b_fc1)\n",
    "    \n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    W_fc2 = weight_variable([1024,size_of_box])\n",
    "    b_fc2 = bias_variable([size_of_box])\n",
    "    \n",
    "    ########end of train#######\n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop,W_fc2)+b_fc2\n",
    "    y_conv = tf.div(1., 1.+tf.exp(-y_conv))\n",
    "    \n",
    "    result_y = tf.Variable(y_collect)\n",
    "    #cost = -tf.reduce_mean(result_y*tf.log(tf.clip_by_value(y_conv,1e-4,0.9999))\n",
    "    #                       + (1-result_y)*tf.log(tf.clip_by_value(1-y_conv,1e-4,0.9999)))\n",
    "    cost = tf.squared_difference(result_y,y_conv)\n",
    "    \n",
    "    ############################ compare result_y:output and y_conv:from layer\n",
    "    \n",
    "    #############setting optimizer############\n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf.constant(0.001))\n",
    "    train = optimizer.minimize(cost)\n",
    "    \n",
    "def TensorboardSetting():\n",
    "    ## Jaesung : TensorBoard Config\n",
    "    logs_path = \"/tmp/mnist/2\" ###########\n",
    "    \n",
    "    # create a summary for our cost and accuracy\n",
    "    tf.scalar_summary(\"cost\", cost)\n",
    "    \n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "\n",
    "## Jaesung : Split the Session and the function to run the tensorboard\n",
    "def LetTheGameBegin():    \n",
    "    \n",
    "    TensorboardSetting()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)    \n",
    "    \n",
    "    writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    x_collect=np.asarray(x_collect)/255\n",
    "    \n",
    "    #########################\n",
    "    \n",
    "    print(\"number of one in result_y\",sess.run(result_y).tolist().count(1))\n",
    "    \n",
    "    for epoch in range(81):\n",
    "        \n",
    "        if epoch%20==0:\n",
    "            print(\"epoch\",epoch)\n",
    "            tmpy = sess.run(y_conv,feed_dict = {x:[x_collect],y:[y_collect],keep_prob:1})\n",
    "            cst = sess.run(cost,feed_dict = {x:[x_collect],y:[y_collect],keep_prob:1})\n",
    "            print(tmpy,sum(sum(cst)))\n",
    "            #print(yconv[0].tolist().count(1))\n",
    "            #print(yconv)\n",
    "            #print(cst)\n",
    "            #print(sum(elem>0.9 for elem in cst[0]))\n",
    "        if epoch%40==0:\n",
    "            res = sess.run(y_conv,feed_dict={x:[x_collect],y:[y_collect],keep_prob:1})\n",
    "            num_of_one = sum(1 if x>=0.5 else 0 for x in res[0])   \n",
    "            print(num_of_one)\n",
    "             \n",
    "        _, summary = sess.run([train, summary_op],feed_dict = {x:[x_collect],y:[y_collect],keep_prob:0.5})\n",
    "        \n",
    "        # write log\n",
    "        writer.add_summary(summary, epoch)\n",
    "        \n",
    "        \n",
    "    with open(\"yconvyconv.txt\",\"w\") as f:\n",
    "        f.write(str(num_of_one))\n",
    "        for x in res[0]:\n",
    "            f.write(str(x))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of one in result_y 1570\n",
      "epoch 0\n",
      "[[ 0.80356163  0.69991922  0.12504664 ...,  0.06981612  0.64787352\n",
      "   0.64216268]] 2406.20747321\n",
      "4051\n",
      "epoch 20\n",
      "[[ 0.43830687  0.31148145  0.0724543  ...,  0.13796642  0.23245154\n",
      "   0.21409386]] 766.238967248\n",
      "epoch 40\n",
      "[[ 0.31681448  0.18290071  0.07488801 ...,  0.08363367  0.14786766\n",
      "   0.1437853 ]] 204.856350442\n",
      "1620\n",
      "epoch 60\n",
      "[[ 0.1420846   0.11199069  0.07430907 ...,  0.06621341  0.10384792\n",
      "   0.08187535]] 58.4321047366\n",
      "epoch 80\n",
      "[[ 0.0998162   0.10038374  0.06895668 ...,  0.03700372  0.06592863\n",
      "   0.0618178 ]] 18.9261657374\n",
      "1571\n"
     ]
    }
   ],
   "source": [
    "train_nn_for_organism_detection(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}